{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Receiver - Consume Instrument Data, perform any necessary aggregations and save to Cassandra\n",
    "Each instrument has its own Kafka topic. The Spark app name is the same as the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOPIC = 'dmi'\n",
    "# TOPIC = 'imu'\n",
    "# TOPIC = 'gps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.ui.port=4040 --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0,com.datastax.spark:spark-cassandra-connector_2.11:2.0.0-M3 pyspark-shell'\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "import pyspark.sql.functions as f\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(TOPIC) \\\n",
    "    .setMaster(\"local[2]\") \\\n",
    "    .set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "sc = SparkContext(conf=conf) \n",
    "sqlContext=SQLContext(sc)\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "\n",
    "from utils import parse_line_into_schema\n",
    "\n",
    "from fastavro import writer, reader, parse_schema\n",
    "import avro_schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a script you would use args and parse them below, Use debug = True in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PARSE ARGS AND SET PATHS, TOPICS\n",
    "KEYSPACE = 'raw_instrument'\n",
    "DATA_OUT_DIR = '/data/'\n",
    "DMI_OUT_FILEPATH = DATA_OUT_DIR + 'dmi.csv'\n",
    "IMU_OUT_FILEPATH = DATA_OUT_DIR + 'imu.csv'\n",
    "LIDAR_OUT_FILEPATH = DATA_OUT_DIR + 'lidar.csv'\n",
    "GPS_OUT_FILEPATH = DATA_OUT_DIR + 'gps.csv'\n",
    "INSTRUMENTS = ['imu', 'dmi', 'lidar', 'gps']\n",
    "\n",
    "if debug:\n",
    "    instrument = TOPIC\n",
    "else:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-i', '--instrument', help='which instrument data to save to disk {0}'.format(INSTRUMENTS))\n",
    "    args = parser.parse_args()\n",
    "    instrument = args.instrument.lower()\n",
    "    \n",
    "if 'dmi' in instrument:\n",
    "    filepath = DMI_OUT_FILEPATH\n",
    "    schema = avro_schemas.dmi\n",
    "    field_names = [d['name'] for d in schema['fields']]\n",
    "    TOPIC = 'dmi'\n",
    "elif 'imu' in instrument:\n",
    "    filepath = IMU_OUT_FILEPATH\n",
    "    schema = avro_schemas.imu\n",
    "    field_names = [d['name'] for d in schema['fields']]\n",
    "    TOPIC = 'imu'\n",
    "elif 'lidar' in instrument:\n",
    "    filepath = LIDAR_OUT_FILEPATH\n",
    "    schema = avro_schemas.lidar\n",
    "    field_names = [d['name'] for d in schema['fields']]\n",
    "    TOPIC = 'lidar'\n",
    "elif 'gps' in instrument:\n",
    "    filepath = GPS_OUT_FILEPATH\n",
    "    schema = avro_schemas.gps\n",
    "    field_names = [d['name'] for d in schema['fields']]\n",
    "    TOPIC = 'gps'\n",
    "else:\n",
    "    print('ERROR: ARGUMENT {0} NOT IN INSTRUMENTS {1}'.format(args.instrument, INSTRUMENTS))\n",
    "    assert False\n",
    "\n",
    "parsed_schema = parse_schema(schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The schema we expect to read given the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__fastavro_parsed': True,\n",
       " 'fields': [{'name': 'coordinate_id', 'type': 'int'},\n",
       "  {'name': 'record_no', 'type': 'int'},\n",
       "  {'name': 'left_total', 'type': 'int'},\n",
       "  {'name': 'right_total', 'type': 'int'},\n",
       "  {'name': 'left', 'type': 'int'},\n",
       "  {'name': 'right', 'type': 'int'}],\n",
       " 'name': 'dmi.avro.sidewalk_rig',\n",
       " 'type': 'record'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SaveToCassandra\n",
    "Save each aggregated row or rows to Cassandra. The topicname is the table name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saveToCassandra(rows):\n",
    "    if not rows.isEmpty(): \n",
    "        sqlContext.createDataFrame(rows).write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .mode('append')\\\n",
    "        .options(table=TOPIC, keyspace=KEYSPACE)\\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Streaming Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 5)\n",
    "kvs = KafkaUtils.createStream(ssc, \"127.0.0.1:2181\", \"spark-streaming-consumer\", {TOPIC: 1})\n",
    "data = kvs.map(lambda x: json.loads(x[1]))\n",
    "rows = data.map(lambda x: x)\n",
    "rows.foreachRDD(saveToCassandra)\n",
    "rows.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-05-01 13:35:05\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/spark/python/pyspark/sql/session.py:340: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-05-01 13:35:10\n",
      "-------------------------------------------\n",
      "{u'right': 150, u'record_no': 314, u'left_total': 42197, u'right_total': 42328, u'coordinate_id': 1556717695294473984, u'left': 142}\n",
      "{u'right': 150, u'record_no': 315, u'left_total': 42344, u'right_total': 42478, u'coordinate_id': 1556717697295793920, u'left': 147}\n",
      "{u'right': 149, u'record_no': 316, u'left_total': 42494, u'right_total': 42627, u'coordinate_id': 1556717699298831872, u'left': 150}\n",
      "{u'right': 143, u'record_no': 317, u'left_total': 42644, u'right_total': 42770, u'coordinate_id': 1556717701300987904, u'left': 150}\n",
      "{u'right': 150, u'record_no': 318, u'left_total': 42787, u'right_total': 42920, u'coordinate_id': 1556717703303450112, u'left': 143}\n",
      "{u'right': 149, u'record_no': 319, u'left_total': 42937, u'right_total': 43069, u'coordinate_id': 1556717705306098944, u'left': 150}\n",
      "{u'right': 145, u'record_no': 320, u'left_total': 43087, u'right_total': 43214, u'coordinate_id': 1556717707308756224, u'left': 150}\n",
      "{u'right': 142, u'record_no': 321, u'left_total': 43237, u'right_total': 43356, u'coordinate_id': 1556717709312016896, u'left': 150}\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-01 13:35:15\n",
      "-------------------------------------------\n",
      "{u'right': 150, u'record_no': 322, u'left_total': 43377, u'right_total': 43506, u'coordinate_id': 1556717711314995968, u'left': 140}\n",
      "{u'right': 150, u'record_no': 323, u'left_total': 43520, u'right_total': 43656, u'coordinate_id': 1556717713315747072, u'left': 143}\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-01 13:35:20\n",
      "-------------------------------------------\n",
      "{u'right': 147, u'record_no': 324, u'left_total': 43670, u'right_total': 43803, u'coordinate_id': 1556717715317027072, u'left': 150}\n",
      "{u'right': 147, u'record_no': 325, u'left_total': 43820, u'right_total': 43950, u'coordinate_id': 1556717717319777024, u'left': 150}\n",
      "{u'right': 150, u'record_no': 326, u'left_total': 43957, u'right_total': 44100, u'coordinate_id': 1556717719322918912, u'left': 137}\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-01 13:35:25\n",
      "-------------------------------------------\n",
      "{u'right': 147, u'record_no': 327, u'left_total': 44107, u'right_total': 44247, u'coordinate_id': 1556717721326024960, u'left': 150}\n",
      "{u'right': 147, u'record_no': 328, u'left_total': 44257, u'right_total': 44394, u'coordinate_id': 1556717723328738048, u'left': 150}\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-01 13:35:30\n",
      "-------------------------------------------\n",
      "{u'right': 147, u'record_no': 329, u'left_total': 44407, u'right_total': 44541, u'coordinate_id': 1556717725330863104, u'left': 150}\n",
      "{u'right': 150, u'record_no': 330, u'left_total': 44549, u'right_total': 44691, u'coordinate_id': 1556717727338536960, u'left': 142}\n",
      "{u'right': 143, u'record_no': 331, u'left_total': 44699, u'right_total': 44834, u'coordinate_id': 1556717729340694784, u'left': 150}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2019-05-01 13:35:35\n",
      "-------------------------------------------\n",
      "{u'right': 144, u'record_no': 332, u'left_total': 44849, u'right_total': 44978, u'coordinate_id': 1556717731343833856, u'left': 150}\n",
      "{u'right': 145, u'record_no': 333, u'left_total': 44999, u'right_total': 45123, u'coordinate_id': 1556717733346854912, u'left': 150}\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-01 13:35:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2019-05-01 13:35:45\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cassandra table content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dmi'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster()\n",
    "session = cluster.connect(KEYSPACE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = session.execute('SELECT * FROM {0} LIMIT 20'.format(TOPIC))\n",
    "for user_row in rows:\n",
    "    print map(lambda k: (k, getattr(user_row, k)), keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- coordinate_id: long (nullable = true)\n",
      " |-- left: integer (nullable = true)\n",
      " |-- left_total: integer (nullable = true)\n",
      " |-- record_no: integer (nullable = true)\n",
      " |-- right: integer (nullable = true)\n",
      " |-- right_total: integer (nullable = true)\n",
      "\n",
      "+-------------------+----+----------+---------+-----+-----------+\n",
      "|      coordinate_id|left|left_total|record_no|right|right_total|\n",
      "+-------------------+----+----------+---------+-----+-----------+\n",
      "|1556717709312016896| 150|     43237|      321|  142|      43356|\n",
      "|1556716972354115840| 150|      1455|       10|  139|       1431|\n",
      "|1556717723328738048| 150|     44257|      328|  147|      44394|\n",
      "|1556717006398350080| 150|      3971|       27|   55|       3806|\n",
      "|1556716988370327040| 141|      2625|       18|  150|       2611|\n",
      "|1556716970351784960| 146|      1305|        9|  150|       1292|\n",
      "|1556717705306098944| 150|     42937|      319|  149|      43069|\n",
      "|1556716980362796032| 148|      2042|       14|  150|       2028|\n",
      "|1556717721326024960| 150|     44107|      327|  147|      44247|\n",
      "|1556717703303450112| 143|     42787|      318|  150|      42920|\n",
      "|1556717715317027072| 150|     43670|      324|  147|      43803|\n",
      "|1556717727338536960| 142|     44549|      330|  150|      44691|\n",
      "|1556716996376455936| 149|      3224|       22|  150|       3205|\n",
      "|1556717711314995968| 140|     43377|      322|  150|      43506|\n",
      "|1556717713315747072| 143|     43520|      323|  150|      43656|\n",
      "|1556716978360126976| 146|      1894|       13|  150|       1878|\n",
      "|1556716994374032128| 150|      3075|       21|  148|       3055|\n",
      "|1556716968350195968| 150|      1159|        8|  147|       1142|\n",
      "|1556717697295793920| 147|     42344|      315|  150|      42478|\n",
      "|1556716990371873024| 150|      2775|       19|  148|       2759|\n",
      "+-------------------+----+----------+---------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.registerTempTable('tmp');\n",
    "data.printSchema()\n",
    "data = sqlContext.sql('select * from tmp')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Record count\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
